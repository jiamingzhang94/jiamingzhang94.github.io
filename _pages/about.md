---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a Research Fellow at **Nanyang Technological University (NTU)**, working with [Prof. Wei Yang Bryan Lim](https://sites.google.com/view/wyb/home). Prior to this, I was a postdoc at **Hong Kong University of Science and Technology (HKUST)**, working with [Prof. Dit-Yan Yeung](https://sites.google.com/view/dyyeung). I received my Ph.D. from **Beijing Jiaotong University** under the supervision of [Prof. Jitao Sang](http://faculty.bjtu.edu.cn/9129/).

My research focuses on **Trustworthy AI**, with particular interests in **adversarial robustness** and **privacy-preserving machine learning**. Recently, I have been exploring these topics in the context of **Multimodal Large Language Models (MLLMs)**.

<div style="padding: 15px; border: 1px solid #e0e0e0; background-color: #f9f9f9; border-radius: 8px; margin-top: 20px; margin-bottom: 20px;">
  <span style="font-weight: bold; font-size: 1.1em;">üî• Opening for Students</span>
  <br>
  We are actively seeking self-motivated students to join our team. Opportunities are available for <strong>prospective PhD students</strong> (full-time), as well as <strong>current PhD students</strong> seeking <strong>visiting positions</strong> (including CSC-funded).
  <br><br>
  If you are interested, please send your CV to <a href="mailto:lanzhang1107@gmail.com">lanzhang1107@gmail.com</a> or directly contact <a href="https://sites.google.com/view/wyb/contact">Prof. Lim</a>.
</div>

<span class='anchor' id='news'></span>
# üî• News

- *2025.09*: Our survey paper **"Safety at Scale"** is published in **Foundations and Trends¬Æ in Privacy and Security**.
- *2025.06*: I joined **NTU** as a Research Fellow.
- *2025.03*: One paper is accepted by **ACL 2025 Findings**.
- *2025.02*: Two papers are accepted by **CVPR 2025**.
- *2025.01*: One paper is accepted by **IEEE TIFS 2025**.

<span class='anchor' id='publications'></span>
# üìù Selected Publications

<div style="margin-bottom: 20px;"></div>

**Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety**
<br> Xingjun Ma, Yifeng Gao, Yixu Wang, ... <u><strong>Jiaming Zhang</strong></u>, ... Tianwei Zhang
<br> ``Foundations and Trends¬Æ in Privacy and Security 2025`` &nbsp; <a href="https://www.nowpublishers.com/article/Details/SEC-051">[Paper]</a> &nbsp; <a href="https://arxiv.org/abs/2502.05206">[ArXiv]</a>

<div style="margin-bottom: 15px;"></div>

**Anyattack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models**
<br> <u><strong>Jiaming Zhang</strong></u>, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Yunhao Chen, Jitao Sang, Dit-Yan Yeung
<br> ``CVPR 2025`` &nbsp; <a href="https://arxiv.org/abs/2410.05346">[Paper]</a> &nbsp; <a href="https://github.com/jiamingzhang94/AnyAttack">[Code]</a> &nbsp; <a href="https://jiamingzhang94.github.io/anyattack/">[Project Page]</a>

<div style="margin-bottom: 15px;"></div>

**TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models**
<br> Xin Wang, Kai Chen, <u><strong>Jiaming Zhang</strong></u>, Jingjing Chen, Xingjun Ma
<br> ``CVPR 2025`` &nbsp; <a href="https://arxiv.org/abs/2411.13136">[Paper]</a>

<div style="margin-bottom: 15px;"></div>

**Investigating and Enhancing Vision-Audio Capability in Omnimodal Large Language Models**
<br> Rui Hu, Delai Qiu, Shuyu Wei, <u><strong>Jiaming Zhang</strong></u>, Yining Wang, Shengping Liu, Jitao Sang
<br> ``ACL Findings 2025`` &nbsp; <a href="https://arxiv.org/pdf/2503.00059">[Paper]</a>

<div style="margin-bottom: 15px;"></div>

**MF-CLIP: Leveraging CLIP as Surrogate Models for No-box Adversarial Attacks**
<br> <u><strong>Jiaming Zhang</strong></u>, Lingyu Qiu, Qi Yi, Yige Li, Jitao Sang, Changsheng Xu, Dit-Yan Yeung
<br> ``IEEE TIFS 2025`` &nbsp; <a href="https://ieeexplore.ieee.org/abstract/document/11126893">[Paper]</a> &nbsp; <a href="https://github.com/jiamingzhang94/MFCLIP">[Code]</a>

<div style="margin-bottom: 15px;"></div>

**Adversarial Prompt Tuning for Vision-Language Models**
<br> <u><strong>Jiaming Zhang</strong></u>, Xingjun Ma, Xin Wang, Lingyu Qiu, Jiaqi Wang, Yu-Gang Jiang, Jitao Sang
<br> ``ECCV 2024`` &nbsp; <a href="https://arxiv.org/abs/2311.11261">[Paper]</a> &nbsp; <a href="https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning">[Code]</a>

<div style="margin-bottom: 15px;"></div>

**Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples**
<br> <u><strong>Jiaming Zhang</strong></u>, Xingjun Ma, Qi Yi, Jitao Sang, Yu-Gang Jiang, Yaowei Wang, Changsheng Xu
<br> ``CVPR 2023`` &nbsp; <a href="https://arxiv.org/abs/2301.01217">[Paper]</a> &nbsp; <a href="https://github.com/jiamingzhang94/Unlearnable-Clusters">[Code]</a>

<div style="margin-bottom: 15px;"></div>

**ImageNet Pre-training also Transfers Non-robustness**
<br> <u><strong>Jiaming Zhang</strong></u>, Jitao Sang, Qi Yi, Yunfan Yang, Huiwen Dong, Jian Yu
<br> ``AAAI 2023`` &nbsp; <a href="https://arxiv.org/abs/2106.10989">[Paper]</a> &nbsp; <a href="https://github.com/jiamingzhang94/ImageNet-Pretraining-transfers-non-robustness">[Code]</a>

<div style="margin-bottom: 15px;"></div>

**Towards Adversarial Attack on Vision-Language Pre-training Models**
<br> <u><strong>Jiaming Zhang</strong></u>, Qi Yi, Jitao Sang
<br> ``ACM MM 2022`` &nbsp; <a href="https://arxiv.org/abs/2206.09391">[Paper]</a> &nbsp; <a href="https://github.com/adversarial-for-goodness/Co-Attack">[Code]</a>

<div style="margin-bottom: 15px;"></div>

**Benign adversarial attack: Tricking algorithm for goodness**
<br> Jitao Sang, Xian Zhao, <u><strong>Jiaming Zhang</strong></u>, Zhiyu Lin
<br> ``ACM MM 2022`` &nbsp; <a href="https://arxiv.org/abs/2107.11986">[Paper]</a>

<div style="margin-bottom: 15px;"></div>

**Robust CAPTCHAs towards malicious OCR**
<br> <u><strong>Jiaming Zhang</strong></u>, Jitao Sang, Kaiyuan Xu, Shangxi Wu, Xian Zhao, Yanfeng Sun, Yongli Hu, Jian Yu
<br> ``IEEE TMM 2021`` &nbsp; <a href="https://ieeexplore.ieee.org/abstract/document/9158388">[Paper]</a>

<div style="margin-bottom: 15px;"></div>

**Adversarial privacy-preserving filter**
<br> <u><strong>Jiaming Zhang</strong></u>, Jitao Sang, Xian Zhao, Xiaowen Huang, Yanfeng Sun, Yongli Hu
<br> ``ACM MM 2020`` &nbsp; <a href="https://arxiv.org/abs/2007.12861">[Paper]</a> &nbsp; <a href="https://github.com/adversarial-for-goodness/APF">[Code]</a>


<span class='anchor' id='honors'></span>
# ü•á Honors and Awards

- *2024*: ACM China Doctoral Dissertation Award Nominee (**Top 5 in China**)
- *2024*: SIGMM China Doctoral Dissertation Award
- *2023*: Zhixing Award (Ê†°ÈïøÂ•ñÂ≠¶Èáë) Nominee
- *2022*: China PhD National Scholarship (ÂçöÂ£´Á†îÁ©∂ÁîüÂõΩÂÆ∂Â•ñÂ≠¶Èáë)
- *2022*: **Rank #1** (1/178) in AISC 2022 Facial Recognition Security Track

<span class='anchor' id='talks'></span>
# üí¨ Invited Talks

- *Nov 2025*: **Security and Privacy on Modern MLLMs**, Institute of Science Tokyo, Tokyo, Japan.
- *Apr 2025*: **Trustworthy AI - Adversarial Attacks & Robustness**, Nanyang Technological University, Singapore.
- *Oct 2021*: **Trustworthy Multimedia Analysis**, ACM MM 2021 Tutorial, Chengdu, China.

<span class='anchor' id='services'></span>
# üåü Academic Services

- **Journal Reviewer:** TPAMI, TIP, TMLR, TCSVT, TIST
- **Conference PC Member:** NeurIPS, ICLR, ICML, CVPR, ICCV, ECCV, ACM MM, AAAI, IJCAI

<br>
<div style="width: 200px; margin: 0 auto;">
  <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=VAtlPDTmWZg5LNhxQiNGugG-iS-TTaZCiidSfPYO7uk&cl=ffffff&w=a"></script>
</div>